{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d9de2c-18f3-482a-8f93-d4638cf5b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import random\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2c478-ea6d-43f9-a2af-b289eda47555",
   "metadata": {},
   "source": [
    "## Crosscut Class 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7936140a-01bb-4b53-9453-94b1ab0faba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crosscut:\n",
    "    # 교차 편집에 필요한 변수 초기화\n",
    "    def __init__(self, dist_obj, video_path, output_path):\n",
    "        self.videos_path = video_path\n",
    "        self.output_path = output_path #mp4 파일\n",
    "        self.min_time = 1000.0\n",
    "        video_num = len(os.listdir(self.videos_path))\n",
    "        self.start_times = [0]*video_num\n",
    "        self.window_time = 10\n",
    "        self.padded_time = 4\n",
    "        self.dist_obj = dist_obj\n",
    "        self.audioclip = None\n",
    "        self.extracted_clips_array = []\n",
    "        self.con_clips = []\n",
    "    \n",
    "    # 사용할 무대영상의 시작점 정렬하기\n",
    "    def video_alignment(self):\n",
    "        for i in range(len(os.listdir(self.videos_path))):\n",
    "            video_path = os.path.join(self.videos_path, sorted(os.listdir(self.videos_path))[i])\n",
    "            clip = VideoFileClip(video_path)\n",
    "            clip = clip.subclip(self.start_times[i], clip.duration)\n",
    "            if self.min_time > clip.duration:\n",
    "                self.audioclip = clip.audio\n",
    "                self.min_time = clip.duration\n",
    "            self.extracted_clips_array.append(clip)\n",
    "        print(\"LOGGER-- {} Video Will Be Mixed\".format(len(self.extracted_clips_array)))\n",
    "            \n",
    "\n",
    "    # 다음 영상 선택하기\n",
    "    def select_next_clip(self, t, current_idx):\n",
    "        # 거리 측정에 필요한 변수 초기화하기\n",
    "        cur_t = t\n",
    "        next_t = min(t + self.window_time, self.min_time)\n",
    "        \n",
    "        reference_clip = self.extraced_clips_array[current_idx].subclip(cur_t, next_t)\n",
    "        d = float(\"Inf\")\n",
    "        cur_clip = None\n",
    "        min_idx = (current_idx+1)%len(self.extracted_clips_array)\n",
    "        \n",
    "        # 비교 영상들과 연재 영상의 거리 측정하기\n",
    "        for video_idx in range(len(self.extracted_clips_array)):\n",
    "            if video_idx == current_idx:\n",
    "                continue\n",
    "            clip = self.extracted_clips_array[video_idx].subclip(cur_t, next_t)\n",
    "            cur_d, plus_frame = self.dist_obj.distance(reference_clip, clip)\n",
    "            print(current_idx, video_idx, cur_d, cur_t + plus_frame)\n",
    "            if d > cur_d:\n",
    "                d = cur_d\n",
    "                min_idx = video_idx\n",
    "                next_t = cur_t + plus_frame\n",
    "                cur_clip = reference_clip.subclip(0, plus_frame)\n",
    "                \n",
    "        # 다음 교차편집 지점 전까지 현재 영상 저장하기\n",
    "        if cur_clip:\n",
    "            clip = cur_clip\n",
    "        else:\n",
    "            clip = reference_clip\n",
    "        self.con_clips.append(clip)\n",
    "        \n",
    "        t = next_t\n",
    "        return t, min_idx\n",
    "            \n",
    "    # 선택한 다음 영상의 padding 추가하기\n",
    "    def add_padding(self, t, next_idx):\n",
    "        print(\"idx : {}\".format(next_idx))\n",
    "        pad_clip = self.extracted_clips_array[next_idx].subclip(t, min(self.min_tiem, t+self.padded_time))\n",
    "        return t, next_idx\n",
    "    \n",
    "    # 교차편집 결과물 저장하기\n",
    "    def write_video(self):\n",
    "        final_clip = concatenate_videoclips(self.con_clips)\n",
    "        if self.audioclip != None:\n",
    "            print(\"Not None\")\n",
    "            final_clip.audio = self.audioclip\n",
    "        final_clip.write_videofile(self.output_path)\n",
    "        return final_clip\n",
    "    \n",
    "    # 교차편집 만들기(메인 함수)\n",
    "    def generate_video(self):\n",
    "        # 영상 전처리하기\n",
    "        self.video_alignment()\n",
    "        t = 3\n",
    "        current_idx = 0\n",
    "        self.con_clips.append(self.extracted_clips_array[current_idx].subclip(0, min(t, int(self.min_time))))\n",
    "        # 노래 끝까지 교차 편집 만들기\n",
    "        while t < int(self.min_time):\n",
    "            t, min_idx = self.select_next_clip(t, current_idx)\n",
    "            t, current_idx = self.add_padding(t,min_idx)\n",
    "        # 교차 편집 결과 영상 저장하기\n",
    "        final_clip = self.write_video()\n",
    "        return final_clip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f2dbe-0d08-4e32-88f6-40243d244d03",
   "metadata": {},
   "source": [
    "## Distance Class 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d5b4e-f9b7-4141-8845-afe34d3a9a48",
   "metadata": {},
   "source": [
    "### RandomDistance Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a52108b9-0120-4634-90ca-12ae0423b1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDistance:\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        dur_end = min(reference_clip.duration, compare_clip.duration)\n",
    "        return random.randrange(1,100), min(dur_end, random.randrange(3,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbded27-bfaa-43bb-8aa2-43af52781562",
   "metadata": {},
   "source": [
    "### FaceDistance Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c8eaf97-96cf-4f01-99ec-9c8fcbbbd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDistance:\n",
    "    def __init__(self, shape_predictor_path, face_embedding_penalty=None):\n",
    "        self.skip_frame_rate = 4 # 거리를 계산할 frame 주기\n",
    "        self.minimax_frames = 5 # 정해진 편집점 주변 중 가장 자연러운 지점을 찾기 위해 둘러볼 주변 프레임 \n",
    "        self.shape_predictor = shape_predictor_path # openCV model\n",
    "        self.face_embedding_pealty = face_embedding_penalty # 얼굴이 다를 때 더해지는 penalty\n",
    "        \n",
    "    def extract_lamdmark(self, reference_clip, compare_clip):\n",
    "        # 영상 저장 및 face landmark detect model 불러오기\n",
    "        self.clips = [reference_clip, compare_clip]\n",
    "        detector = dlib.get_frontal_face_detector()\n",
    "        predictor = dlib.shape_predictor(self.shape_predictor)\n",
    "        clips_frame_info = []\n",
    "        for clip in self.clips:\n",
    "            # 각 영상의 정보를 저장하기 위해 loop마다 초기화 하기\n",
    "            i = 0\n",
    "            every_frame_info=[]\n",
    "            while True:\n",
    "                # 각 영상에서 face Landmark 얻기\n",
    "                frame = clip.get_frame(i*1.0/clip.fps)\n",
    "                i += self.skip_frame_rate\n",
    "                if (i*1.0/clip.fps) > clip.duration:\n",
    "                    break\n",
    "                frame = imutils.resize(frame, width=800)\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                rects = detector(gray, 0)\n",
    "                # 얻은 face landmark를 가공해서 every_frame_info에 저장하기\n",
    "                if len(rects) > 0:\n",
    "                    max_width = 0\n",
    "                    max_rect = None\n",
    "                    for rect in rects:\n",
    "                        if int(rects[0].width()) > max_width:\n",
    "                            max_rect = rect\n",
    "                    shape = predictor(gray, max_rect)\n",
    "                    shape = face_utils.shape_to_np(shape)\n",
    "                    every_frame_info.append(shape)\n",
    "                else:\n",
    "                    every_frame_info.append([])\n",
    "            # 영상 frame별 landmark 정보를 clips_frame_info에 저장하기\n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "        cv2.destroyAllWindows()\n",
    "        return clips_frame_info\n",
    "        \n",
    "    def embedding_cosine_distance(self, reference_frame, compare_frame):\n",
    "        face_detector = MTCNN(select_largest=True)\n",
    "        embed_model = InceptionResnetV1(pretrained='vggface2').eval()\n",
    "        \n",
    "        reference_frame = np.array(reference_frame)\n",
    "        compare_frame = np.array(compare_frame)\n",
    "        \n",
    "        try:\n",
    "            reference_frame_detected = face_detector(reference_frame)\n",
    "            compare_frame_detected = face_detector(compare_frame)\n",
    "        except:\n",
    "            cosine_dist = 1\n",
    "            return cosine_dist\n",
    "        \n",
    "        reference_frame_embed = embed_model(reference_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        compare_frame_embed = embed_model(compare_frame_detected.unsqueeze(0)).detach().numpy()\n",
    "        reference_frame_embed = np.squeeze(compare_frame_embed)\n",
    "        cosine_dist = 1 - np.dot(reference_frame_embed, compare_frame_embed) / (np.linalg.norm(reference_frame_embed)) * (np.linalg.norm(compare_frame_embed))\n",
    "        \n",
    "        return cosine_dist\n",
    "    \n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = []\n",
    "        for i in range(min_size-1):\n",
    "            if len(clips_frame_info[0][i] > 0 and len(clips_frame_info[1][i+1]) > 0):\n",
    "                l = 36\n",
    "                r = 45\n",
    "                left_eye = ((clips_frame_info[0][i][l][0] - clips_frame_info[1][i+1][l][0])**2\n",
    "                             + (clips_frame_info[0][i][l][1] - clips_frame_info[1][i+1][l][1])**2)**0.5\n",
    "                right_eye = ((clips_frame_info[0][i][r][0] - clips_frame_info[1][i+1][r][0])**2 \n",
    "                             + (clips_frame_info[0][i][r][1] - clips_frame_info[1][i+1][r][1])**2)**0.5\n",
    "                total_diff = left_eye + right_eye\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "    \n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "        # 거리 계산에 필요한 정보들 먼저 얻기\n",
    "        clips_frame_info = self.extract_lamdmark(reference_clip, compare_clip)\n",
    "        min_size = min(len(clips_frame_info[0]), len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "        clips = [reference_clip, compare_clip]\n",
    "        minimax_frames = self.minimax_frames\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0\n",
    "        \n",
    "        # 최소 거리가 되는 영상과 시간 찾기\n",
    "        for i in range(min_size - (minimax_frames-1)):\n",
    "            start_minmax_idx = 0 if (i - minimax_frames)<0 else i - minimax_frames\n",
    "            \n",
    "            if (None not in dist_arr[start_minmax_idx:i+minimax_frames]):\n",
    "                tmp_max = np.max(dist_arr[start_minmax_idx:i+minimax_frames])\n",
    "                if min_diff > tmp_max:\n",
    "                    min_diff = tmp_max\n",
    "                    min_idx = i\n",
    "                    \n",
    "        # Face Embedding Penalty 추가하기\n",
    "        if self.face_embedding_penalty != None and min_diff < np.float(\"Inf\"):\n",
    "            ref_frame = reference_clip.gget_frame(min_idx*1.0/reference_clip.fps)\n",
    "            frame = compare_clip.get_frame(min_idx * 1.0/compare_clip.fps)\n",
    "            cosine_dist = self.embedding_cosine_distance(ref_frame, frame)\n",
    "            min_diff += consine_dist * self.face_embedding_penalty\n",
    "            \n",
    "        # 두 영상 간의 최소 거리 정보 Return\n",
    "        return min_diff, (min_idx*self.skip_frame_rate)/self.clips[0].fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61531a29-385c-4242-adda-e8a93bf89b76",
   "metadata": {},
   "source": [
    "### PoseDistance Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bed0f985-9e1a-4cbd-a23e-e7ccb5b41050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoseDistance:\n",
    "    def __init__(self):\n",
    "        self.SKIP_FRAME_RATE = 10\n",
    "        self.MINIMAX_FRAME = 4\n",
    "        self. model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "        self.model.eval()\n",
    "        os.environ['KMP_DUPLICATE_LIB_OK'] = \"True\"\n",
    "\n",
    "    # 가수 위치를 파악하는 함수\n",
    "    def extract_boxes(self, reference_clip, compare_clip):\n",
    "        #변수 초기화\n",
    "        self.clips = [reference_clip, compare_clip]\n",
    "        clips_frame_info = []\n",
    "\n",
    "        for clip in self.clips:\n",
    "            # 각 영상의 정보를 저장하기 위해 loop마다 초기화하기\n",
    "            i = 0\n",
    "            every_frame_info = []\n",
    "            while True:\n",
    "                # Faster RCNN을 이용해 물체 판별하기\n",
    "                i+=self.SKIP_FRAME_RATE\n",
    "                if (i*1.0/clip.fps) > clip.duration:\n",
    "                    break\n",
    "\n",
    "                frame = clip.get_frame(i*1.0/clip.fps)\n",
    "                frame = imutils.resize(frame, width=640)\n",
    "                frame = frame/255\n",
    "                frame = np.transpose(frame, (2,0,1))\n",
    "                x = [torch.from_numpy(frame).float()]\n",
    "                predictions = self.model(x)\n",
    "                prediction = predictions[0]\n",
    "                # 판별정보 재가공하기\n",
    "                each_box_list = zip(prediction['boxes'].tolis(), prediction['labels'].tolist(), prediction['scores'].tolist())\n",
    "                filtered_box_list = filter(lambda x: x[1]==1 and x[2] >= 0.95, each_box_list)\n",
    "                filtered_center_dot_list = list(map(lambda x:[(x[0][0]+x[0][2])/2, (x[0][1]+x[0][3])/2], filtered_box_list))\n",
    "                sorted_dot_list = sorted(filtered_center_dot_list, key = lambda x:x[0])\n",
    "\n",
    "                # 재가공한 정보 every_frame_info에 저장하기\n",
    "                every_frame_info.append(sorted_dot_list)\n",
    "            # 영상 frame별 landmark 정보 clips_frame_info에 저장하기\n",
    "            clips_frame_info.append(np.array(every_frame_info))\n",
    "        return clips_frame_info\n",
    "\n",
    "    # 두 영상의 frame 거리를 계산하는 함수\n",
    "    def get_all_frame_distance(self, clips_frame_info, min_size):\n",
    "        dist_arr = list()\n",
    "        for i in range(min_size):\n",
    "            if len(clips_frame_info[0][i]) > 0 and len(clips_frame_info[1][i]) > 0:\n",
    "                ref_frame_dots = clips_frame_info[0][i]\n",
    "                compare_frame_dots = clips_frame_info[1][i]\n",
    "                min_dot_num = min(len(ref_frame_dots), len(compare_frame_dots))\n",
    "                dot_num_diff = abs(len(ref_frame_dots) - len(compare_frame_dots))\n",
    "                panalty = ((self.clips[0].w **2 + self.clips[0].h**2)**0.5) * abs(len(res_frame_dots) - len(compare_frame_dots))\n",
    "                total_diff = penalty*dot_num_diff\n",
    "\n",
    "                for dot_idx in range(min_dot_num):\n",
    "                    total_diff += ((ref_frame_dots[dot_idx][0] - compare_frame_dots[dot_idx][0])**2 + (ref_frame_dots[dot_idx][1] - compare_frame_dots[dot_idx][1])**2)**0.5\n",
    "\n",
    "                dist_arr.append(total_diff)\n",
    "            else:\n",
    "                dist_arr.append(None)\n",
    "        return dist_arr\n",
    "\n",
    "    # 거리 측정 함수\n",
    "    def distance(self, reference_clip, compare_clip):\n",
    "\n",
    "        # 거리 계산에 필요한 정보들 얻기\n",
    "        clips_frame_info = self.extract_boxes(reference_clip, compare_clip)\n",
    "        min_size = min(len(clips_frame_info[0]), len(clips_frame_info[1]))\n",
    "        dist_arr = self.get_all_frame_distance(clips_frame_info, min_size)\n",
    "        min_diff = np.float('Inf')\n",
    "        min_idx = 0\n",
    "\n",
    "        for i in range(min_size-(self.MINIMAX_FRAME-1)):\n",
    "            start_minmax_idx = 0 if (i - self.MINIMAX_FRAME) < 0 else i - self.MINIMAX_FRAME\n",
    "            if (None not in dist_arr[start_minmax_idx : i + self.MINIMAX_FRAME]):\n",
    "                tmp_max = np.max(dist_arr[i:i+self.MINIMAX_FRAME])\n",
    "            if min_diff > tmp_max:\n",
    "                min_diff = tmp_max\n",
    "                min_idx = i\n",
    "        return min_diff, (min_idx*self.SKIP_FRAME_RATE)/reference_clip.fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267a786-3d25-4ef4-90a2-57830e27f66d",
   "metadata": {},
   "source": [
    "## 교차 편집 실행 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b491ab86-8621-4ba1-be58-7914a9322ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/my_stagemix.mp4\n",
      "LOGGER-- 0 Video Will Be Mixed\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c24398dc018d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcross_cut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrosscut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mcross_cut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-f0502e8bfbcb>\u001b[0m in \u001b[0;36mgenerate_video\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mcurrent_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcon_clips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextracted_clips_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m# 노래 끝까지 교차 편집 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "method = 'face'\n",
    "video_path = '../data/fifth_season'\n",
    "output_path = '../data/my_stagemix.mp4'\n",
    "shape_predictor_path = '../data/shape_predictor_68_face_landmarks.dat'\n",
    "face_embedding_panalty = 100\n",
    "\n",
    "print(output_path)\n",
    "if method == 'random':\n",
    "    distance = RandomDistance()\n",
    "    \n",
    "elif method == 'face':\n",
    "    distance = FaceDistance(shape_predictor_path, face_embedding_panalty)\n",
    "\n",
    "elif method == 'pose':\n",
    "    distance = PoseDistance()\n",
    "    \n",
    "cross_cut = Crosscut(distance, video_path, output_path)\n",
    "cross_cut.generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225bac3-7a63-4c30-9c36-e07fc64bc894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c07b8-1bef-422f-a87d-1a397e253c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505e9cb-c901-44a2-9f10-ca546b294a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f67372-fab7-4060-852b-095a07618a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c383ea7-126d-437f-a0e8-7db3960bf77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d571c1-e068-48b5-9188-7587fa859c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41eedae-1f77-4631-8d3f-733e79c1442c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a820dc-cb49-4813-a21c-dd7a1e47372b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741ecd7-abc6-4e09-88de-67198688dd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5904e501-d13b-4e50-ac05-1e25da6e7a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b417e-fdb3-43fe-88ea-6b305c768f2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94fa5be-603d-491c-a3f6-b3644e4746c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf192a6-4e96-4287-a905-58e2301c1bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bfd88a-6337-4510-8246-8a7658e68fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934d643f-c38f-4ad8-b577-ee205d8300de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e0f87-5997-43e3-ae8b-d8fb18393247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cca186-4418-41bf-b3a7-e1f62d3271f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af88a4d-c7e8-4225-be7f-de5f3eb6e7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20c704f-86ac-477e-9ef0-5b480dc71d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4ff1a-9ace-41ac-b80d-393c2f96e982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830544c-e51d-4d5f-bd55-ec47381aa9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95608470-a68c-46c5-b6ef-21e9ee2b1d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664565d7-b9d9-4dee-bf72-dc68fed634d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98df014-16de-47b8-9af5-b066093ac7c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a9fec-4f95-4a48-8978-310bd5293286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833677e8-e5db-4c71-88d3-3064468cbd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622b28a-f982-4382-ac49-d7559e81272e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ysher_yonsei",
   "language": "python",
   "name": "ysher_yonsei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
