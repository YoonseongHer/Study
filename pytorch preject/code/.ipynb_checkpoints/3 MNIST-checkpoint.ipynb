{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4115b060-a762-4302-9b33-f760aa1f8cd6",
   "metadata": {},
   "source": [
    "## 3.2.2 CNN 손글씨 숫자 이미지 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e089913-1eb5-4cfb-be80-68b966b7c93c",
   "metadata": {},
   "source": [
    "### 기본 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80c159a-72e8-487c-8185-51262cbe84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import  torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import font_manager, rc, rcParams\n",
    "font_name = font_manager.FontProperties(fname='/usr/share/fonts/truetype/nanum/NanumSquareR.ttf').get_name()\n",
    "rc('font', family=font_name)\n",
    "rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e36ee-e862-40b9-bd35-1d38789b7f57",
   "metadata": {},
   "source": [
    "### 분석 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ea84f0-7d0c-4935-b104-09154610cba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current cuda device is cuda\n"
     ]
    }
   ],
   "source": [
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if is_cuda else \"cpu\")\n",
    "\n",
    "print(\"Current cuda device is\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315037a3-225d-4a27-90e0-23505812197f",
   "metadata": {},
   "source": [
    "### Hyperparameter 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfac8a8b-11cb-4062-8efa-cb278dd1bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "epoch_num = 15\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50e150-4a53-43ed-9cbf-77853d1d6c39",
   "metadata": {},
   "source": [
    "### data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e26b4a7f-02fb-4719-9842-c25ffe860238",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root = './data', train = True, download = True, transform = transforms.ToTensor())\n",
    "test_data = datasets.MNIST(root = './data', train = False, transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b233ab4-dac3-4714-9109-e033a2036e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data: 60000\n",
      "number of training data: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"number of training data:\", len(train_data))\n",
    "print(\"number of training data:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd3aa62-a546-48de-adfb-5afc7a633623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = train_data[0]\n",
    "plt.imshow(image.squeeze().numpy(), cmap=\"gray\")\n",
    "plt.title(\"label : %s\" % label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c94bf9-3440-49e4-904f-baab3cb0c9be",
   "metadata": {},
   "source": [
    "### 미니 배치 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30fcc8c4-0732-4142-96f1-1ba3c6b2635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name            | type                      | size\n",
      "Num of Batch    |                           | 1200\n",
      "first_batch     | <class 'list'>            | 2\n",
      "first_batch[0]  | <class 'torch.Tensor'>    | torch.Size([50, 1, 28, 28])\n",
      "first_batch[1]  | <class 'torch.Tensor'>    | torch.Size([50, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "first_batch = train_loader.__iter__().__next__()\n",
    "print(\"{:15s} | {:<25} | {}\".format(\"name\",\"type\",\"size\"))\n",
    "print(\"{:15s} | {:<25} | {}\".format(\"Num of Batch\",\"\",len(train_loader)))\n",
    "print(\"{:15s} | {:<25} | {}\".format('first_batch',str(type(first_batch)),len(frist_batch)))\n",
    "print(\"{:15s} | {:<25} | {}\".format(\"first_batch[0]\", str(type(first_batch[0])),first_batch[0].shape))\n",
    "print(\"{:15s} | {:<25} | {}\".format(\"first_batch[1]\", str(type(first_batch[1])),first_batch[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594311bb-3b80-467c-92f1-3d6e3a1ab0a5",
   "metadata": {},
   "source": [
    "### model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd916507-d68f-4c8a-ac17-1c8753eb75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,3,1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216,128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x,2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x,dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7613a2e-70a4-4456-b305-6b76d26a310f",
   "metadata": {},
   "source": [
    "### optimizer, loss function define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fe921d6-5361-4fa1-98f5-ff2d083cbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "568c37bb-d9e9-42d9-908c-c0494301278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d877270-6e72-4b4b-84d4-dfd2c0d889f9",
   "metadata": {},
   "source": [
    "### model learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6902541a-e91a-4bfe-a3f0-e7fea1612b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 0\\Total_Loss: 2.294\n",
      "Train Step: 1000\\Total_Loss: 0.434\n",
      "Train Step: 2000\\Total_Loss: 0.139\n",
      "Train Step: 3000\\Total_Loss: 0.204\n",
      "Train Step: 4000\\Total_Loss: 0.093\n",
      "Train Step: 5000\\Total_Loss: 0.344\n",
      "Train Step: 6000\\Total_Loss: 0.090\n",
      "Train Step: 7000\\Total_Loss: 0.090\n",
      "Train Step: 8000\\Total_Loss: 0.089\n",
      "Train Step: 9000\\Total_Loss: 0.074\n",
      "Train Step: 10000\\Total_Loss: 0.017\n",
      "Train Step: 11000\\Total_Loss: 0.173\n",
      "Train Step: 12000\\Total_Loss: 0.012\n",
      "Train Step: 13000\\Total_Loss: 0.075\n",
      "Train Step: 14000\\Total_Loss: 0.046\n",
      "Train Step: 15000\\Total_Loss: 0.007\n",
      "Train Step: 16000\\Total_Loss: 0.043\n",
      "Train Step: 17000\\Total_Loss: 0.044\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "i = 0\n",
    "for epoch in range(epoch_num):\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Train Step: {}\\Total_Loss: {:.3f}\".format(i, loss.item()))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6dcd4-fc62-44d9-a7a3-bba4c68023d5",
   "metadata": {},
   "source": [
    "### model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ee9631d-80c4-4adc-96f8-316dfdacd6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Accuracy: 98.83%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data = data.to(device)\n",
    "    target = target.to(device)\n",
    "    \n",
    "    output = model(data)\n",
    "    prediction = output.data.max(1)[1]\n",
    "    correct += prediction.eq(target.data).sum()\n",
    "    \n",
    "print(\"Test set: Accuracy: {:.2f}%\".format(100 * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ysher_yonsei",
   "language": "python",
   "name": "ysher_yonsei"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
